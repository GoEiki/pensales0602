const transcriptEl = document.getElementById("transcript");
const statusEl = document.getElementById("status");

let recognition;
let transcript = "";
let currentStep = 0;

const prompts = [
  ["#1ã‚’å‡ºåŠ›ã—ã¦", "#2ã‚’å‡ºåŠ›ã—ã¦"],
  ["ãƒ•ã‚©ãƒ­ãƒ¼", "æ¬¡ã®è©±é¡Œ"],
  ["#3ã‚’å‡ºåŠ›ã—ã¦", "#4ã‚’å‡ºåŠ›ã—ã¦"],
  ["ãƒ•ã‚©ãƒ­ãƒ¼", "æ¬¡ã®è©±é¡Œ"],
  ["#5ã‚’å‡ºåŠ›ã—ã¦", "#6ã‚’å‡ºåŠ›ã—ã¦"],
  ["ãƒ•ã‚©ãƒ­ãƒ¼", "æ¬¡ã®è©±é¡Œ"]
];

// éŸ³å£°èªè­˜ã®åˆæœŸåŒ–
if ('webkitSpeechRecognition' in window) {
  recognition = new webkitSpeechRecognition();
  recognition.lang = 'ja-JP';
  recognition.continuous = true;
  recognition.interimResults = true;

  recognition.onresult = (event) => {
    let finalTranscript = "";
    for (let i = event.resultIndex; i < event.results.length; ++i) {
      finalTranscript += event.results[i][0].transcript;
    }
    transcript = finalTranscript;
    transcriptEl.textContent = transcript;
  };

  recognition.onerror = (e) => {
    console.error("éŸ³å£°èªè­˜ã‚¨ãƒ©ãƒ¼:", e);
    if (e.error === "aborted") return;  // â˜…ä¿®æ­£ãƒã‚¤ãƒ³ãƒˆ: abortã¯ç„¡è¦–
    if (["no-speech", "audio-capture", "not-allowed"].includes(e.error)) {
      recognition.stop();
      setTimeout(() => recognition.start(), 500);
    }
  };

  recognition.onend = () => {
    console.log("èªè­˜çµ‚äº†"); // â˜…ä¿®æ­£ãƒã‚¤ãƒ³ãƒˆ: è‡ªå‹•å†é–‹ã¯å‰Šé™¤ï¼ˆç«¶åˆé˜²æ­¢ï¼‰
    // recognition.start(); â† ã“ã“å‰Šé™¤
  };

  recognition.start();
} else {
  alert("ã“ã®ãƒ–ãƒ©ã‚¦ã‚¶ã§ã¯webkitSpeechRecognitionãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚");
}

// WebSocketæ¥ç¶š
const ws = new WebSocket(`ws://${location.hostname}:3001`);
ws.onopen = () => console.log("WebSocket connected");

// ã‚­ãƒ¼å…¥åŠ›å‡¦ç†
document.addEventListener("keydown", async (event) => {
  if (event.key !== 'a' && event.key !== 'b') return;

  const key = event.key;
  const pair = prompts[currentStep];
  if (!pair) return;

  const isFollowPhase = pair[0] === "ãƒ•ã‚©ãƒ­ãƒ¼" && pair[1] === "æ¬¡ã®è©±é¡Œ";

  let promptToSend = null;

  if (isFollowPhase) {
    if (key === 'a') {
      promptToSend = "åº—å“¡ã®è©±ã—ãŸå†…å®¹ã‚’ãƒ•ã‚©ãƒ­ãƒ¼ã—ã¦";
    } else {
      ws.send(JSON.stringify({ action: "next", key }));
      currentStep++;
      statusEl.textContent = "æ¬¡ã®è©±é¡Œã¸";
      transcript = "";
      transcriptEl.textContent = "";
      return;
    }
  } else {
    promptToSend = key === 'a' ? pair[0] : pair[1];
  }

  const combined = `${promptToSend}ã€‚${transcript || ""}`; // transcriptãŒç©ºã§ã‚‚OK
  statusEl.textContent = "é€ä¿¡ä¸­...";
  
// document.addEventListener("keydown", async (event) => {
//   if (event.key !== 'a' && event.key !== 'b') return;

//   // ğŸ› ï¸ ä¿®æ­£ï¼šaã‚­ãƒ¼ã®ã¨ãã ã‘ transcript ã‚’ãƒã‚§ãƒƒã‚¯
//   if (event.key === 'a' && !transcript) return;

//   const key = event.key;
//   const pair = prompts[currentStep];
//   let promptToSend = null;

//   if (pair[0] === "ãƒ•ã‚©ãƒ­ãƒ¼" && pair[1] === "æ¬¡ã®è©±é¡Œ") {
//     if (key === 'a') {
//       promptToSend = "åº—å“¡ã®è©±ã—ãŸå†…å®¹ã‚’ãƒ•ã‚©ãƒ­ãƒ¼ã—ã¦";
//     } else {
//       ws.send(JSON.stringify({ action: "next", key }));
//       currentStep++;
//       statusEl.textContent = "æ¬¡ã®è©±é¡Œã¸";
//       transcript = "";
//       transcriptEl.textContent = "";
//       return;
//     }
//   }
//    else {
//     promptToSend = key === 'a' ? pair[0] : pair[1];
//   }

//   const combined = `${promptToSend}ã€‚${transcript}`;
//   statusEl.textContent = "é€ä¿¡ä¸­...";

  try {
    // éŸ³å£°èªè­˜ã‚’ä¸€æ™‚åœæ­¢ï¼ˆæ˜ç¤ºçš„ã«æ­¢ã‚ã¦ã‹ã‚‰èª­ã¿ä¸Šã’ï¼‰
    console.log("ğŸ¤ éŸ³å£°èªè­˜ï¼šåœæ­¢ä¸­ï¼ˆèª­ã¿ä¸Šã’ã®ãŸã‚ï¼‰");
    recognition.abort(); // â˜…ä¿®æ­£ãƒã‚¤ãƒ³ãƒˆ: stop() â†’ abort()ï¼ˆonendã‚¤ãƒ™ãƒ³ãƒˆæŠ‘æ­¢ï¼‰

    const res = await fetch('/gpt', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ text: combined })
    });

    const data = await res.json();
    const reply = data.reply;

    // èª­ã¿ä¸Šã’
    const utter = new SpeechSynthesisUtterance(reply);
    utter.lang = 'ja-JP';
    utter.rate = 1.3;
    utter.pitch = 1.1;
    utter.volume = 1.0;

    utter.onend = () => {
      console.log("ğŸ—£ï¸ èª­ã¿ä¸Šã’çµ‚äº† â†’ 100mså¾Œã«éŸ³å£°èªè­˜å†é–‹");
      setTimeout(() => {
        try {
          recognition.start();  //å†é–‹ã¯ã“ã“ã ã‘
          statusEl.textContent = "å¾…æ©Ÿä¸­";
        } catch (err) {
          console.warn("âš ï¸ èªè­˜ã‚¹ã‚¿ãƒ¼ãƒˆå¤±æ•—:", err);
        }
      }, 100);  //0.1ç§’å¾…ã£ã¦ã‹ã‚‰å†é–‹
    };

    utter.onerror = () => {
      console.error("èª­ã¿ä¸Šã’ã‚¨ãƒ©ãƒ¼");
      setTimeout(() => recognition.start(), 100);
      statusEl.textContent = "ã‚¨ãƒ©ãƒ¼ï¼ˆèª­ã¿ä¸Šã’å¤±æ•—ï¼‰";
    };

    speechSynthesis.speak(utter);

    if (!(pair[0] === "ãƒ•ã‚©ãƒ­ãƒ¼" && pair[1] === "æ¬¡ã®è©±é¡Œ" && key === 'a')) {
      currentStep++;
      ws.send(JSON.stringify({ action: "next", key }));
    }

    transcript = "";
    transcriptEl.textContent = "";

  } catch (err) {
    console.error("é€ä¿¡ã‚¨ãƒ©ãƒ¼:", err);
    statusEl.textContent = "ã‚¨ãƒ©ãƒ¼";
  }
});